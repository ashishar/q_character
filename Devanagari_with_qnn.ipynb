{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishar/q_character/blob/main/Devanagari_with_qnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlUHdpeR9L6F",
        "outputId": "3221101a-e12e-4de0-dc7a-fc13b9602a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchvision is already installed.\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchvision\n",
        "# !pip install qiskit-machine-learning\n",
        "import importlib\n",
        "\n",
        "# Check if torchvision is installed\n",
        "try:\n",
        "    importlib.import_module('torchvision')\n",
        "    print(\"torchvision is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"torchvision is not installed. Installing...\")\n",
        "    # Install torchvision using pip\n",
        "    try:\n",
        "        import pip\n",
        "        pip.main(['install', 'torchvision'])\n",
        "        print(\"torchvision installed successfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while installing torchvision:\", str(e))\n",
        "\n",
        "# try:\n",
        "#     importlib.import_module('qiskit')\n",
        "#     print(\"qiskit is already installed.\")\n",
        "# except ImportError:\n",
        "#     print(\"qiskit is not installed. Installing...\")\n",
        "#     # Install torchvision using pip\n",
        "#     try:\n",
        "#         import pip\n",
        "#         pip.main(['install', 'qiskit'])\n",
        "#         print(\"qiskit installed successfully.\")\n",
        "#     except Exception as e:\n",
        "#         print(\"Error occurred while installing qiskit:\", str(e))\n",
        "\n",
        "# try:\n",
        "#     importlib.import_module('qiskit_machine_learning')\n",
        "#     print(\"qiskit-machine-learning is already installed.\")\n",
        "# except ImportError:\n",
        "#     print(\"qiskit-machine-learning is not installed. Installing...\")\n",
        "#     # Install torchvision using pip\n",
        "#     try:\n",
        "#         import pip\n",
        "#         pip.main(['install', 'qiskit-machine-learning'])\n",
        "#         print(\"qiskit-machine-learning installed successfully.\")\n",
        "#     except Exception as e:\n",
        "#         print(\"Error occurred while installing qiskit-machine-learning:\", str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to the desired location where the file is stored.\n",
        "%cd /content/drive/MyDrive/\n",
        "\n",
        "# Import the file named quantum_circuit_simulator.py\n",
        "#import quantum_circuit_simulator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeGiH98DbAsy",
        "outputId": "4c90ff95-c0bb-4f7f-98c9-45d74c48672e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "def get_device(gpu_no):\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda', gpu_no)\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class quantum_circuit:\n",
        "\n",
        "    def __init__(self, num_qubits : int, state_vector = None, device = 'cuda', gpu_no = 0):\n",
        "\n",
        "        \"\"\"\n",
        "        Defines a quantum circuit object that stores the full state-vector (evolved through\n",
        "        the unitary operations of a quantum circuit) of `num_qubits` number of qubits.\n",
        "\n",
        "        Args:\n",
        "            num_qubits (int): Number of qubits in the circuit.\n",
        "\n",
        "            state_vector (torch.Tensor, optional): The full state vector of the quantum circuit.\n",
        "                                          Defaults to None. If None is provided then the state\n",
        "                                          vector is automatically initialized to the ket |0000...0>.\n",
        "\n",
        "            device (str, optional): Device on which the state vector should be stored (CPU / GPU).\n",
        "                                    Defaults to 'cuda' i.e. GPU.\n",
        "\n",
        "            gpu_no (int, optional): If there are multiple GPUs then this parameter defines which\n",
        "                                    GPU to use. Defaults to 0 i.e. the first device.\n",
        "        \"\"\"\n",
        "        #----------------------------------------------------------------------------------------\n",
        "\n",
        "        if device != 'cuda':\n",
        "            self.device = torch.device(device)\n",
        "        else:\n",
        "            self.device = get_device(gpu_no)\n",
        "\n",
        "\n",
        "        #----------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "        self.n = num_qubits   # number of qubits\n",
        "        self.dim = 2**self.n  # dimention of the n-qubit hilbert space\n",
        "\n",
        "\n",
        "        #----------------------------------------------------------------------------------------\n",
        "\n",
        "        '''\n",
        "        state_vector can\n",
        "        (1) either be a vector of shape (dim,)\n",
        "        (2) either be a matrix of shape (dim, number of examples)\n",
        "        '''\n",
        "\n",
        "        if state_vector is None:\n",
        "            ''' Initialize the state-vector to |0000...0> '''\n",
        "            state_vector = torch.zeros(self.dim, device=self.device, dtype=torch.cfloat)\n",
        "            state_vector[0] = 1\n",
        "            self.state_vector = state_vector.reshape(-1,1)\n",
        "        else:\n",
        "            if state_vector.shape[0] == self.dim:\n",
        "                ''' state_vector must be normalized '''\n",
        "                self.state_vector = state_vector.to(torch.cfloat)\n",
        "            else:\n",
        "                print('The dimension 2**n does NOT match the shape of the state vector. n is the number of qubits.')\n",
        "\n",
        "\n",
        "        #----------------------------------------------------------------------------------------\n",
        "\n",
        "        # single qubit Pauli gates (matrices) :\n",
        "        self.I        = torch.tensor([[1,   0], [0,  1]], device=self.device, dtype=torch.cfloat)\n",
        "        self.x_matrix = torch.tensor([[0.,  1], [1,  0]], device=self.device, dtype=torch.cfloat)\n",
        "        self.y_matrix = torch.tensor([[0, -1j], [1j, 0]], device=self.device, dtype=torch.cfloat)\n",
        "        self.z_matrix = torch.tensor([[1,   0], [0, -1]], device=self.device, dtype=torch.cfloat)\n",
        "\n",
        "        self.h_matrix = (1 / math.sqrt(2)) * torch.tensor([[1, 1], [1, -1]], device=self.device, dtype=torch.cfloat)\n",
        "\n",
        "\n",
        "        # single qubit projectors :\n",
        "        self.proj_0 = torch.tensor([[1, 0], [0, 0]], device=self.device, dtype=torch.cfloat)\n",
        "        self.proj_1 = torch.tensor([[0, 0], [0, 1]], device=self.device, dtype=torch.cfloat)\n",
        "\n",
        "\n",
        "\n",
        "   #======================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "    def single_qubit_gate(self, target : int, gate : torch.Tensor):\n",
        "        \"\"\"\n",
        "        Applies a single qubit gate = I ⊗ I ⊗ ... ⊗ gate ⊗ ... ⊗ I\n",
        "\n",
        "        Args:\n",
        "        target (int): The qubit index on which the gate will be applied\n",
        "        gate (torch.Tensor): The matrix representation of a single qubit gate\n",
        "\n",
        "        Returns:\n",
        "        The state vector of the full quantum circuit after applying the single qubit gate.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if target < 0 or self.n <= target:\n",
        "            print('0 <= traget <= num_qubits - 1 is NOT satisfied!')\n",
        "\n",
        "        else:\n",
        "            single_q_gate = torch.tensor(1, device=self.device, dtype=torch.cfloat) # initialize\n",
        "\n",
        "            for k in range(self.n):\n",
        "                if k == target:\n",
        "                    single_q_gate = torch.kron(single_q_gate, gate)\n",
        "                else:\n",
        "                    single_q_gate = torch.kron(single_q_gate, self.I)\n",
        "\n",
        "            #------------------------------------------------------\n",
        "\n",
        "            self.state_vector = torch.matmul(single_q_gate, self.state_vector)\n",
        "            return self.state_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def controlled_gate(self, control: int, target: int, gate : torch.Tensor):\n",
        "        \"\"\"\n",
        "        Applies a two-qubit controlled gate between the 'control` and `target` qubits.\n",
        "\n",
        "        control_gate_part_0 = I ⊗ |0><0| ⊗ ... ⊗ I    ⊗ ... ⊗ I\n",
        "        control_gate_part_1 = I ⊗ |1><1| ⊗ ... ⊗ gate ⊗ ... ⊗ I         SEE: the control is set to 1\n",
        "\n",
        "        control_gate = control_gate_part_0 + control_gate_part_1\n",
        "\n",
        "\n",
        "        Args:\n",
        "        control (int): Control qubit index\n",
        "        target (int):  Target qubit index\n",
        "        gate (torch.Tensor): The matrix representation of a single qubit gate\n",
        "\n",
        "        Returns:\n",
        "        The state vector of the full quantum circuit after applying the two-qubit gate.\n",
        "        \"\"\"\n",
        "\n",
        "        if control < 0 or self.n <= control:\n",
        "            print('0 <= control <= num_qubits - 1 is NOT satisfied!')\n",
        "        elif target < 0 or self.n <= target:\n",
        "            print('0 <= target <= num_qubits - 1 is NOT satisfied!')\n",
        "        elif control == target:\n",
        "            print('control and traget qubits must be different!')\n",
        "        else:\n",
        "            control_gate_part_0 = torch.tensor(1, device=self.device, dtype=torch.cfloat) # initialize\n",
        "            control_gate_part_1 = torch.tensor(1, device=self.device, dtype=torch.cfloat)\n",
        "\n",
        "            for k in range(self.n):\n",
        "                if k == control:\n",
        "                    control_gate_part_0 = torch.kron(control_gate_part_0, self.proj_0)\n",
        "                    control_gate_part_1 = torch.kron(control_gate_part_1, self.proj_1)\n",
        "                elif k == target:\n",
        "                    control_gate_part_0 = torch.kron(control_gate_part_0, self.I)\n",
        "                    control_gate_part_1 = torch.kron(control_gate_part_1, gate)\n",
        "                else:\n",
        "                    control_gate_part_0 = torch.kron(control_gate_part_0, self.I)\n",
        "                    control_gate_part_1 = torch.kron(control_gate_part_1, self.I)\n",
        "\n",
        "            control_gate = control_gate_part_0 + control_gate_part_1\n",
        "\n",
        "            self.state_vector = torch.matmul(control_gate, self.state_vector)\n",
        "            return self.state_vector\n",
        "\n",
        "\n",
        "    #======================================================================================================\n",
        "\n",
        "    def x(self, target : int):                           # Applies X gate (matrix) on the target qubit\n",
        "        'NOTE: 0 <= target <= num_qubits - 1'\n",
        "        self.single_qubit_gate(target, self.x_matrix)\n",
        "\n",
        "\n",
        "    def y(self, target : int):\n",
        "        self.single_qubit_gate(target, self.y_matrix)\n",
        "\n",
        "\n",
        "    def z(self, target : int):\n",
        "        self.single_qubit_gate(target, self.z_matrix)\n",
        "\n",
        "\n",
        "    def h(self, target : int):                           # Applies Hadamard gate (matrix) on the target qubit\n",
        "        self.single_qubit_gate(target, self.h_matrix)\n",
        "\n",
        "\n",
        "   #======================================================================================================\n",
        "\n",
        "\n",
        "    def Rx(self, target : int, theta):\n",
        "\n",
        "        \"\"\"\n",
        "        Applies Rx gate (rotation around x axis) on the target qubit\n",
        "\n",
        "        Args:\n",
        "        theta (torch.Tensor): Angle by which the qubit should be rotated around X axis.\n",
        "                              Usually a tunable parameter is passed.\n",
        "\n",
        "        target (int): Qubit index on which the Rx gate will be applied.\n",
        "        NOTE: 0 <= target <= num_qubits - 1\n",
        "        \"\"\"\n",
        "\n",
        "        co = torch.cos(theta / 2)\n",
        "        si = torch.sin(theta / 2)\n",
        "        self.Rx_matrix = torch.stack([torch.stack([co, -1j*si]), torch.stack([-1j*si, co])])\n",
        "\n",
        "        self.single_qubit_gate(target, self.Rx_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def Ry(self, target : int, theta):            #like Rx, Ry gate applies (rotation around y axis) on the target qubit\n",
        "\n",
        "        co = torch.cos(theta / 2)\n",
        "        si = torch.sin(theta / 2)\n",
        "        self.Ry_matrix = torch.stack([torch.stack([co, -si]), torch.stack([si, co])])\n",
        "\n",
        "        self.single_qubit_gate(target, self.Ry_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def Rz(self, target : int, theta):            #like Rx, Ry gate applies (rotation around z axis) on the target qubit\n",
        "\n",
        "        exp_theta = torch.exp( 1j*theta )\n",
        "        zero = torch.tensor(0)\n",
        "        one = torch.tensor(1)\n",
        "        self.Rz_matrix = torch.stack([torch.stack([one, zero]), torch.stack([zero, exp_theta])])\n",
        "\n",
        "        self.single_qubit_gate(target, self.Rz_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def R(self, target : int, theta, phi, lamda):\n",
        "        \"\"\"\n",
        "        Applies general rotation to the target qubit\n",
        "\n",
        "        Args:\n",
        "        theta, phi and lamda (torch.Tensor): The Euler angles which define a general rotation around Bloch sphere.\n",
        "\n",
        "        target (int): Qubit index on which the gate will be applied.\n",
        "        \"\"\"\n",
        "\n",
        "        a =                                  torch.cos(theta / 2)\n",
        "        b =        - torch.exp(1j * lamda) * torch.sin(theta / 2)\n",
        "        c =            torch.exp(1j * phi) * torch.sin(theta / 2)\n",
        "        d =  torch.exp(1j * (phi + lamda)) * torch.cos(theta / 2)\n",
        "        self.R_matrix = torch.stack([torch.stack([a, b]), torch.stack([c, d])])\n",
        "\n",
        "        self.single_qubit_gate(target, self.R_matrix)\n",
        "\n",
        "\n",
        "    #======================================================================================================\n",
        "\n",
        "\n",
        "    def Ry_layer(self, angs: torch.Tensor):\n",
        "        '''\n",
        "        Applies tensor-product of single-qubit rotations around y-axis\n",
        "        '''\n",
        "\n",
        "        cos, sin = torch.cos(angs[0]), torch.sin(angs[0])\n",
        "        '''\n",
        "        Use torch.stack otherwise computation graph will be broken (or will not begin).\n",
        "        And, grad will be gone (will not be stored).\n",
        "        '''\n",
        "        rot = torch.stack([torch.stack([cos, -sin]), torch.stack([sin, cos])])\n",
        "\n",
        "        for i in range(1, len(angs)):                                    # one angles for each qubit\n",
        "            cos, sin = torch.cos(angs[i]), torch.sin(angs[i])\n",
        "            rot = torch.kron(rot, torch.stack([torch.stack([cos, -sin]), torch.stack([sin, cos])]))\n",
        "\n",
        "        #--------------------------------------------------------------------------\n",
        "\n",
        "        self.state_vector = torch.matmul(rot, self.state_vector)      # rotated state vector\n",
        "        return self.state_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def Rz_layer(self, angs: torch.Tensor):       #like Ry_layer, Rz_layer acts\n",
        "\n",
        "        exp_ang = torch.exp( 1j*angs[0] )\n",
        "        zero = torch.tensor(0)\n",
        "        one = torch.tensor(1)\n",
        "\n",
        "        rot = torch.stack([torch.stack([one, zero]), torch.stack([zero, exp_ang])])\n",
        "\n",
        "        for i in range(1, len(angs)):                                    # one angles for each qubit\n",
        "            exp_ang = torch.exp( 1j*angs[i] )\n",
        "            rot = torch.kron(rot, torch.stack([torch.stack([one, zero]), torch.stack([zero, exp_ang])]) )\n",
        "\n",
        "        #--------------------------------------------------------------------------\n",
        "\n",
        "        self.state_vector = torch.matmul(rot, self.state_vector)      # rotated state vector\n",
        "        return self.state_vector\n",
        "\n",
        "\n",
        "\n",
        "    #======================================================================================================\n",
        "\n",
        "    def cx(self, control: int, target: int):\n",
        "        \"\"\"\n",
        "        Applies controlled-X gate = I ⊗ |0><0| ⊗ ... ⊗ I ⊗ ... ⊗ I +\n",
        "                                    I ⊗ |1><1| ⊗ ... ⊗ X ⊗ ... ⊗ I\n",
        "\n",
        "        Args:\n",
        "        control (int): Control qubit index\n",
        "        target (int):  Target qubit index\n",
        "        \"\"\"\n",
        "        self.controlled_gate(control, target, self.x_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def cz(self, control: int, target: int):                           #like cx, cz gate acts\n",
        "        self.controlled_gate(control, target, self.z_matrix)\n",
        "\n",
        "\n",
        "\n",
        "   #======================================================================================================\n",
        "\n",
        "\n",
        "    def cx_linear_layer(self):\n",
        "        '''\n",
        "        Applies cx(n-1,n) ... cx(2,3) cx(1,2) cx(0,1) |state_vector>\n",
        "\n",
        "        NOTE: First cx(0,1) will act on |state_vector>, then cx(1,2)\n",
        "              And in the last cx(n-1,n) will act.\n",
        "              order matter in case of cx\n",
        "        '''\n",
        "\n",
        "        self.controlled_gate(self.n - 2, self.n - 1, self.x_matrix)\n",
        "        for i in range(self.n - 3, -1, -1):\n",
        "            self.controlled_gate(i, i+1, self.x_matrix)\n",
        "\n",
        "\n",
        "\n",
        "    def cz_linear_layer(self):                                        #like cx_linear_layer, cz_linear_layer  acts\n",
        "        self.controlled_gate(self.n - 2, self.n - 1, self.z_matrix)\n",
        "        for i in range(self.n - 3, -1, -1):\n",
        "            self.controlled_gate(i, i+1, self.z_matrix)\n",
        "\n",
        "\n",
        "   #======================================================================================================\n",
        "\n",
        "    def probabilities(self):\n",
        "        \"\"\"\n",
        "        probabilities obtained in the z-measurement (computational basis) on the state vector\n",
        "\n",
        "        Returns: A torch.Tensor of the size same as the state vector\n",
        "        \"\"\"\n",
        "\n",
        "        return self.state_vector.conj() * self.state_vector\n"
      ],
      "metadata": {
        "id": "Q8iVmP8bi--r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to download devanagari character dataset from Kaggle.\n",
        "# !pip install kaggle\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download --force -d rishianand/devanagari-character-set\n",
        "# !unzip \"/content/drive/My Drive/devanagari-character-set.zip\" -d \"/content/data\""
      ],
      "metadata": {
        "id": "Ip8-RV8ec6Z8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w66wRZ_a42gp",
        "outputId": "77e926f8-47c4-4707-b836-f8f4c1cd2616"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_images(dataset):\n",
        "    # Calculating start and end indices for the crop\n",
        "    start = (32 - 28) // 2  # (32 - 28) / 2 = 2\n",
        "    end = start + 28  # 2 + 28 = 30\n",
        "\n",
        "    # Cropping to get 28x28 images\n",
        "    #dataset[:, start:end, start:end] would be the sliced tensor\n",
        "    dataset_out=torch.zeros(92000,28,28)\n",
        "    #print(dataset.shape[0])\n",
        "\n",
        "    for i in range(dataset.shape[0]):\n",
        "      dataset_out=dataset[:, start:end, start:end]\n",
        "#      print(dataset[i].shape, \" \", dataset[i].numpy())\n",
        "\n",
        "    return dataset_out #dataset[:, start:end, start:end]\n",
        "\n",
        "# import torch\n",
        "# from PIL import Image\n",
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "# # Assume `images` is your tensor with shape [100, 32, 32]\n",
        "# images = torch.randn(100, 32, 32)  # Example tensor\n",
        "\n",
        "# def crop_images(tensor_images):\n",
        "#     new_size=(28, 28)\n",
        "#     resized_images = []\n",
        "#     for img_tensor in tensor_images:\n",
        "#         # Convert the tensor to a PIL image\n",
        "#         img_pil = TF.to_pil_image(img_tensor)\n",
        "\n",
        "#         # Resize the image using Pillow\n",
        "#         img_pil = img_pil.resize(new_size, Image.NEAREST)\n",
        "\n",
        "#         # Convert the PIL image back to a tensor\n",
        "#         img_tensor_resized = TF.to_tensor(img_pil)\n",
        "\n",
        "#         # Append the resized tensor to the list\n",
        "#         resized_images.append(img_tensor_resized)\n",
        "#     #print(resized_images.shape)\n",
        "#     # Stack all resized image tensors back into a single tensor\n",
        "#     return torch.stack(resized_images)"
      ],
      "metadata": {
        "id": "1L1wbE6EZB59"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/data/data.csv')\n",
        "\n",
        "# Convert all columns to numeric, coercing errors and handling non-numeric data\n",
        "#data = data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "\n",
        "# Fill NaN values with 0 (consider other strategies depending on your context)\n",
        "#data = data.fillna(0)\n",
        "\n",
        "# Assume images are 28x28 pixels, update this based on your actual image dimensions\n",
        "image_height = 32\n",
        "image_width = 32\n",
        "\n",
        "# Separate features and labels\n",
        "features = data.iloc[:, :-1]  # All rows, all columns except the last one\n",
        "features.head()\n",
        "# column_sums = features.sum()\n",
        "# print(column_sums)\n",
        "\n",
        "# Fill missing values if any\n",
        "#features = features.fillna(0)\n",
        "\n",
        "features_images = features.values.reshape(-1, 32, 32)\n",
        "labels = data.iloc[:, -1]     # All rows, only the last column\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "features_tensor = torch.tensor(features_images, dtype=torch.float32)\n",
        "# Cropping the images\n",
        "cropped_features_tensor = crop_images(features_tensor)\n",
        "cropped_features_tensor=cropped_features_tensor.squeeze(1)\n",
        "labels_tensor = torch.tensor(labels_encoded, dtype=torch.long)\n",
        "\n",
        "print(\"cropped features\", cropped_features_tensor.shape)\n",
        "# Resize all images\n",
        "#resized_images = torch.stack([resize_image(img) for img in features_tensor])\n",
        "#resized_images_squeezed=resized_images.squeeze(1)\n",
        "#print(\"resized_tensor\", resized_images_squeezed.shape)\n",
        "\n",
        "# Convert DataFrame to PyTorch tensors\n",
        "# features_tensor = torch.tensor(features.values, dtype=torch.float32)\n",
        "# labels_tensor = torch.tensor(labels.values, dtype=torch.long)\n",
        "\n",
        "# Create a TensorDataset to be used with a DataLoader\n",
        "dataset = TensorDataset(cropped_features_tensor, labels_tensor)\n",
        "#dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "\n",
        "# Generate a random permutation of indices and select the first 1000\n",
        "indices = torch.randperm(len(dataset))[:10000]\n",
        "\n",
        "# Create a subset with these indices\n",
        "subset = Subset(dataset, indices)\n",
        "\n",
        "# Split the subset into training and testing datasets (80-20 or 70: 30 split)\n",
        "num_train = int(0.7 * len(subset))  # 80% for training\n",
        "num_test = len(subset) - num_train  # Remaining 20% for testing\n",
        "\n",
        "train_dataset, test_dataset = random_split(subset, [num_train, num_test])\n",
        "# def reshape_dataset(dataset, new_shape):\n",
        "#     reshaped_features = [features.view(new_shape) for features, _ in dataset]\n",
        "#     labels = [labels for _, labels in dataset]\n",
        "#     features_tensor = torch.stack(reshaped_features)\n",
        "#     labels_tensor = torch.stack(labels)\n",
        "#     return TensorDataset(features_tensor, labels_tensor)\n",
        "\n",
        "# # Reshape the datasets to 32x32\n",
        "# train_dataset_reshaped = reshape_dataset(train_dataset, (-1, 32, 32))\n",
        "# test_dataset_reshaped = reshape_dataset(test_dataset, (-1, 32, 32))\n",
        "\n",
        "# Create DataLoaders for train and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Example usage\n",
        "# for features, labels in train_loader:\n",
        "#     # Training loop code here\n",
        "#     print(\"Training batch features:\", features.size())\n",
        "#     print(\"Training batch labels:\", labels.size())\n",
        "\n",
        "# for features, labels in test_loader:\n",
        "#     # Testing loop code here\n",
        "    # print(\"Testing batch features:\", features.size())\n",
        "    # print(\"Testing batch labels:\", labels.size())\n",
        "\n",
        "# Create a DataLoader to handle batching\n",
        "#dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpN0fqKy9zGZ",
        "outputId": "51ab0b83-42c4-486c-a595-49b31ba3e1cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cropped features torch.Size([92000, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting function\n",
        "def plot_images(images, labels, num_images=5):\n",
        "  fig, axes = plt.subplots(1, num_images, figsize=(10, 2))\n",
        "  for i, ax in enumerate(axes):\n",
        "    ax.imshow(images[i], cmap='gray')\n",
        "    ax.set_title(f'Label: {labels[i]}')\n",
        "    ax.axis('on')\n",
        "  plt.show()\n",
        "\n",
        "# Display the first 5 images and labels\n",
        "for features, labels in train_loader:\n",
        "  plot_images(features, labels, 5)\n",
        "  break  # Only process the first batch#\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "RQcQp2F5SXnN",
        "outputId": "1670f1f7-1f45-4789-abd4-2fa0b622b3b3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for features, labels in train_loader:\n",
        "  print(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S47c9-faRHiI",
        "outputId": "03fc0271-9b1a-417e-d14d-893b5de040cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([64, 28, 28])\n",
            "torch.Size([24, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "\n",
        "def plot_images_by_label(dataloader, specific_label, num_images=5):\n",
        "    count = 0  # Counter to track how many images we have displayed\n",
        "    plt.figure(figsize=(10, 2))  # Set up the figure size\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        for image, label in zip(images, labels):\n",
        "            if label.item() == specific_label:  # Check if the label matches\n",
        "                count += 1\n",
        "                ax = plt.subplot(1, num_images, count)  # Create a subplot for each image\n",
        "                ax.imshow(image.squeeze(), cmap='gray')  # Display image, squeeze() is used to remove extra dimensions if any\n",
        "                ax.title.set_text(f'Label: {label.item()}')\n",
        "                ax.axis('on')  # Turn off axis\n",
        "\n",
        "                if count == num_images:  # Only display a specific number of images\n",
        "                    plt.show()\n",
        "                    return\n",
        "# Example usage\n",
        "# Assuming your DataLoader, images, and labels are properly set up\n",
        "# Here you call the function with the label you want to display and the number of images\n",
        "\n",
        "for i in range(2):\n",
        "  plot_images_by_label(train_loader, specific_label=i, num_images=5)# Call the function with the desired label\n",
        "  clear_output(wait=True)  # Clear the current output and wait for new output\n",
        "  time.sleep(0.01)  # Pause for a second to simulate time taken in an iteration\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "gePTI1133A1-",
        "outputId": "d4390d20-942b-4a04-c5c3-cdaae3c80769"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"    # Get gpu or cpu device for training\n",
        "print(f\"Using {device} device\\n\")\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "from skimage.feature import hog\n",
        "#from quantum_circuit_simulator import quantum_circuit\n",
        "\n",
        "\n",
        "# Define a function to compute HOG features for an image\n",
        "def compute_hog_features(image):\n",
        "    features, _ = hog(image, orientations=12, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "\n",
        "    return features\n",
        "\n",
        "#=====================================================================================\n",
        "\n",
        "\n",
        "class QNN(torch.nn.Module):                              # Define model\n",
        "    def __init__(self, n, L):                            # number of qubits = n, number of quantum layers = L\n",
        "        super().__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        angles = torch.empty((L, n), dtype=torch.float64)\n",
        "        torch.nn.init.uniform_(angles, -0.01, 0.01)\n",
        "        self.angles = torch.nn.Parameter(angles)                   # it makes angles learnable parameters\n",
        "\n",
        "        # self.fc1 = nn.Linear(1024, 1024)\n",
        "        # self.fc2 = nn.Linear(784,512)\n",
        "        # self.fc3 = nn.Linear(512, 1024)\n",
        "\n",
        "        self.linear1 = nn.Linear(2**n, 256)                          # classical linear layer\n",
        "        self.linear2 = nn.Linear(256,46)                          # classical linear layer\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = F.pad(x, (2 ,2, 2, 2), \"constant\", 0)                  # (left, right, top, bottom) padding\n",
        "        x_projection = torch.sum(x, dim=1).to(device)  # Calculate x-axis projection\n",
        "        #x_projection = x_projection / torch.norm(x_projection, p=1)\n",
        "        #print(\"x projection \",x_projection.shape)\n",
        "        y_projection = torch.sum(x, dim=2).to(device)  # Calculate x-axis projection\n",
        "        #print(\"y projection \",y_projection.shape)\n",
        "        #y_projection = y_projection / torch.norm(y_projection, p=1)\n",
        "        #angle projection\n",
        "        rotated_images1 = TF.rotate(x,30).to(device)\n",
        "        rotated_images2 = TF.rotate(x,45).to(device)\n",
        "        rotated_images3 = TF.rotate(x,60).to(device)\n",
        "\n",
        "        #rotated projections\n",
        "        rotated_projection1=torch.sum(rotated_images1, dim=2).to(device)\n",
        "        rotated_projection2=torch.sum(rotated_images2, dim=2).to(device)\n",
        "        rotated_projection3=torch.sum(rotated_images3, dim=2).to(device)\n",
        "\n",
        "        #rotated_images = torch.sum(torch.stack([TF.rotate(img, angle) for img in inputs]))\n",
        "        x_image=x.cpu().numpy()\n",
        "        hog_features = [compute_hog_features(np.squeeze(image)) for image in x_image]\n",
        "        hog_features_tensor = torch.tensor(np.array(hog_features), dtype=torch.float32).to(device)\n",
        "        #print(hog_features_tensor.shape)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = x[:, :-92]\n",
        "\n",
        "        # print(\"sizes \",x.shape, y_projection.shape, rotated_projection1.shape,rotated_projection2.shape,\n",
        "        #      rotated_projection3.shape,hog_features_tensor.shape)\n",
        "\n",
        "        combined_projection=torch.cat((x,x_projection,y_projection, rotated_projection1,rotated_projection2,rotated_projection3,#rotated_projection4.squeeze(),rotated_projection5.squeeze(),rotated_projection6.squeeze(),\n",
        "                                       hog_features_tensor), dim=1)\n",
        "        #print(x.shape, \" \",hog_features_tensor.shape)\n",
        "        #torch.zeros(x.shape[0],100).to(device)\n",
        "        #print(\"combined \", combined_projection.shape)\n",
        "        #x = self.flatten(x)\n",
        "        combined_projection /= torch.linalg.norm(x.clone(), ord=2, dim=1, keepdim=True)   # L2 normalization to change x --> |x⟩\n",
        "\n",
        "        # combined_projection = torch.sigmoid(self.fc1(combined_projection))\n",
        "        # x1 = torch.sigmoid(self.fc2(x1))\n",
        "        # x1 = torch.sigmoid(self.fc3(x1))\n",
        "\n",
        "        '''initializing parameterized quantum circuits (PQC)'''\n",
        "\n",
        "        qc = quantum_circuit(num_qubits = n, state_vector = combined_projection.T)   # each column is a feature-vector of an example\n",
        "        for l in range(L):\n",
        "            qc.Ry_layer(self.angles[l].to(torch.cfloat))           # rotation part of lth quantum layer\n",
        "            qc.cx_linear_layer()                                   # entangling part of lth quantum layer\n",
        "\n",
        "        'after passing through the PQC, measurement on the output-ket in the computational basis'\n",
        "        x = torch.real(qc.probabilities())               # each column is a probabilities-vector for an example\n",
        "                                                         # x.shape = (dim, batch size)\n",
        "\n",
        "        #print(torch.sum(x, dim=0))                      # to see whether probabilities add up to 1 or not\n",
        "\n",
        "        x = self.linear1(x.T)                            # x.shape = (batch size, 10),  classical linear layer\n",
        "        x= self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Kulr9lBq_jxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb7afa5-5cd1-451e-c107-1781a5abcd22"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def performance_estimate(dataset, model, loss_fn, train_or_test):\n",
        "    '''this function computes accuracy and loss of a model on the training or test set'''\n",
        "    data_size = len(dataset)\n",
        "\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    loss, accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            accuracy += (pred.argmax(1) == y).sum().item()\n",
        "            loss += loss_fn(pred, y).item()\n",
        "    accuracy /= data_size                                            # accuracy lies in the interval [0, 1]\n",
        "    loss /= num_batches\n",
        "    print(f\"{train_or_test} accuracy: {round(accuracy, 3)},  {train_or_test} loss: {round(loss,3)}\")\n",
        "    return accuracy, loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def one_epoch(model, loss_fn, optimizer, train_dataset, test_dataset, batch_size):\n",
        "\n",
        "    A_train, L_train, A_test, L_test = [], [], [], []\n",
        "\n",
        "    dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        out = model(X)                             # Perform a single forward pass\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        optimizer.zero_grad()                      # Clear gradients\n",
        "        loss.backward()                            # Derive gradients, backpropagation\n",
        "        optimizer.step()                           # Update parameters based on gradients\n",
        "\n",
        "\n",
        "        if batch % batch_size == 0:\n",
        "            #As training progress, computing and appending loss and accuracy of the model on train and test set\n",
        "            accuracy_train, loss_train = performance_estimate(train_dataset, model, loss_fn, 'train')\n",
        "            accuracy_test, loss_test = performance_estimate(test_dataset, model, loss_fn, 'test ')\n",
        "            print()\n",
        "\n",
        "            A_train.append(accuracy_train)\n",
        "            L_train.append(loss_train)\n",
        "            A_test.append(accuracy_test)\n",
        "            L_test.append(loss_test)\n",
        "\n",
        "            #print(f\"train loss: {round(loss,3)}\")\n",
        "\n",
        "    return A_train, L_train, A_test, L_test\n",
        "\n",
        "\n",
        "\n",
        "def training(train_dataset, test_dataset, batch_size, n, L, lr_, weight_decay_, epochs):\n",
        "\n",
        "    model = QNN(n=n, L=L).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_, weight_decay=weight_decay_)\n",
        "\n",
        "    A_Train, L_Train, A_Test, L_Test = [], [], [], []\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1} ---------------------------------- \\n\")\n",
        "        #As training progress, computing and appending loss and accuracy of the model on train and test set\n",
        "        A_train, L_train, A_test, L_test = one_epoch(model, loss_fn, optimizer, train_dataset, test_dataset, batch_size)\n",
        "        A_Train += A_train\n",
        "        L_Train += L_train\n",
        "        A_Test += A_test\n",
        "        L_Test += L_test\n",
        "\n",
        "        #accuracy, loss = performance_estimate(test_dataset, model, loss_fn, 'test ')\n",
        "\n",
        "    model_state_dict = model.state_dict()           # for saving or loading the trained model\n",
        "\n",
        "    return A_Train, L_Train, A_Test, L_Test, model_state_dict"
      ],
      "metadata": {
        "id": "VYomaJwMBp2m"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "n = 10\n",
        "dim = 2**n              # dimension of the n-qubit Hilbert space\n",
        "L = 3\n",
        "\n",
        "n_angs = n*L\n",
        "\n",
        "print(\"number of qubits = \", n)\n",
        "print(\"number of quantum layers = \", L)\n",
        "print(f\"number of angles (learnable parameters of quantum circuit) = {n_angs}\\n \")\n",
        "\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "print(f'batch_size = {batch_size}\\n')\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "A_Train, L_Train, A_Test, L_Test, model_state_dict = training(train_dataset, test_dataset, batch_size=batch_size, n=n, L=L,\n",
        "                                                              lr_=0.01/10, weight_decay_=1e-10, epochs=100)\n",
        "\n",
        "\n",
        "print(f' ~~~~~ training is done ~~~~~\\n')"
      ],
      "metadata": {
        "id": "Ogr-wkCNBuJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d45aff-8d31-4b0d-8e96-82a0f6eddcd8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of qubits =  10\n",
            "number of quantum layers =  3\n",
            "number of angles (learnable parameters of quantum circuit) = 30\n",
            " \n",
            "batch_size = 64\n",
            "\n",
            "Epoch 1 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.026,  train loss: 3.824\n",
            "test  accuracy: 0.025,  test  loss: 3.829\n",
            "\n",
            "train accuracy: 0.374,  train loss: 2.689\n",
            "test  accuracy: 0.359,  test  loss: 2.726\n",
            "\n",
            "Epoch 2 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.488,  train loss: 2.125\n",
            "test  accuracy: 0.481,  test  loss: 2.186\n",
            "\n",
            "train accuracy: 0.546,  train loss: 1.775\n",
            "test  accuracy: 0.523,  test  loss: 1.861\n",
            "\n",
            "Epoch 3 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.56,  train loss: 1.673\n",
            "test  accuracy: 0.528,  test  loss: 1.78\n",
            "\n",
            "train accuracy: 0.595,  train loss: 1.536\n",
            "test  accuracy: 0.564,  test  loss: 1.658\n",
            "\n",
            "Epoch 4 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.607,  train loss: 1.481\n",
            "test  accuracy: 0.566,  test  loss: 1.613\n",
            "\n",
            "train accuracy: 0.636,  train loss: 1.382\n",
            "test  accuracy: 0.599,  test  loss: 1.517\n",
            "\n",
            "Epoch 5 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.639,  train loss: 1.35\n",
            "test  accuracy: 0.605,  test  loss: 1.489\n",
            "\n",
            "train accuracy: 0.655,  train loss: 1.284\n",
            "test  accuracy: 0.618,  test  loss: 1.453\n",
            "\n",
            "Epoch 6 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.673,  train loss: 1.238\n",
            "test  accuracy: 0.623,  test  loss: 1.403\n",
            "\n",
            "train accuracy: 0.676,  train loss: 1.205\n",
            "test  accuracy: 0.629,  test  loss: 1.377\n",
            "\n",
            "Epoch 7 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.685,  train loss: 1.171\n",
            "test  accuracy: 0.634,  test  loss: 1.361\n",
            "\n",
            "train accuracy: 0.697,  train loss: 1.137\n",
            "test  accuracy: 0.641,  test  loss: 1.337\n",
            "\n",
            "Epoch 8 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.708,  train loss: 1.104\n",
            "test  accuracy: 0.652,  test  loss: 1.31\n",
            "\n",
            "train accuracy: 0.708,  train loss: 1.08\n",
            "test  accuracy: 0.646,  test  loss: 1.299\n",
            "\n",
            "Epoch 9 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.72,  train loss: 1.048\n",
            "test  accuracy: 0.654,  test  loss: 1.274\n",
            "\n",
            "train accuracy: 0.73,  train loss: 1.016\n",
            "test  accuracy: 0.668,  test  loss: 1.234\n",
            "\n",
            "Epoch 10 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.719,  train loss: 1.023\n",
            "test  accuracy: 0.649,  test  loss: 1.273\n",
            "\n",
            "train accuracy: 0.737,  train loss: 0.982\n",
            "test  accuracy: 0.668,  test  loss: 1.237\n",
            "\n",
            "Epoch 11 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.73,  train loss: 0.984\n",
            "test  accuracy: 0.663,  test  loss: 1.239\n",
            "\n",
            "train accuracy: 0.742,  train loss: 0.946\n",
            "test  accuracy: 0.665,  test  loss: 1.199\n",
            "\n",
            "Epoch 12 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.754,  train loss: 0.92\n",
            "test  accuracy: 0.675,  test  loss: 1.197\n",
            "\n",
            "train accuracy: 0.75,  train loss: 0.922\n",
            "test  accuracy: 0.67,  test  loss: 1.208\n",
            "\n",
            "Epoch 13 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.771,  train loss: 0.877\n",
            "test  accuracy: 0.684,  test  loss: 1.161\n",
            "\n",
            "train accuracy: 0.757,  train loss: 0.872\n",
            "test  accuracy: 0.685,  test  loss: 1.162\n",
            "\n",
            "Epoch 14 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.766,  train loss: 0.864\n",
            "test  accuracy: 0.69,  test  loss: 1.16\n",
            "\n",
            "train accuracy: 0.78,  train loss: 0.832\n",
            "test  accuracy: 0.696,  test  loss: 1.129\n",
            "\n",
            "Epoch 15 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.785,  train loss: 0.816\n",
            "test  accuracy: 0.691,  test  loss: 1.129\n",
            "\n",
            "train accuracy: 0.783,  train loss: 0.804\n",
            "test  accuracy: 0.692,  test  loss: 1.113\n",
            "\n",
            "Epoch 16 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.78,  train loss: 0.81\n",
            "test  accuracy: 0.69,  test  loss: 1.126\n",
            "\n",
            "train accuracy: 0.78,  train loss: 0.799\n",
            "test  accuracy: 0.686,  test  loss: 1.128\n",
            "\n",
            "Epoch 17 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.798,  train loss: 0.767\n",
            "test  accuracy: 0.707,  test  loss: 1.097\n",
            "\n",
            "train accuracy: 0.796,  train loss: 0.765\n",
            "test  accuracy: 0.704,  test  loss: 1.099\n",
            "\n",
            "Epoch 18 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.802,  train loss: 0.759\n",
            "test  accuracy: 0.698,  test  loss: 1.112\n",
            "\n",
            "train accuracy: 0.811,  train loss: 0.722\n",
            "test  accuracy: 0.708,  test  loss: 1.061\n",
            "\n",
            "Epoch 19 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.796,  train loss: 0.75\n",
            "test  accuracy: 0.701,  test  loss: 1.098\n",
            "\n",
            "train accuracy: 0.806,  train loss: 0.714\n",
            "test  accuracy: 0.697,  test  loss: 1.061\n",
            "\n",
            "Epoch 20 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.811,  train loss: 0.706\n",
            "test  accuracy: 0.7,  test  loss: 1.075\n",
            "\n",
            "train accuracy: 0.814,  train loss: 0.7\n",
            "test  accuracy: 0.704,  test  loss: 1.064\n",
            "\n",
            "Epoch 21 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.812,  train loss: 0.685\n",
            "test  accuracy: 0.709,  test  loss: 1.055\n",
            "\n",
            "train accuracy: 0.814,  train loss: 0.678\n",
            "test  accuracy: 0.711,  test  loss: 1.048\n",
            "\n",
            "Epoch 22 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.823,  train loss: 0.663\n",
            "test  accuracy: 0.716,  test  loss: 1.047\n",
            "\n",
            "train accuracy: 0.826,  train loss: 0.658\n",
            "test  accuracy: 0.714,  test  loss: 1.051\n",
            "\n",
            "Epoch 23 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.828,  train loss: 0.644\n",
            "test  accuracy: 0.722,  test  loss: 1.034\n",
            "\n",
            "train accuracy: 0.825,  train loss: 0.643\n",
            "test  accuracy: 0.713,  test  loss: 1.041\n",
            "\n",
            "Epoch 24 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.842,  train loss: 0.616\n",
            "test  accuracy: 0.72,  test  loss: 1.011\n",
            "\n",
            "train accuracy: 0.833,  train loss: 0.618\n",
            "test  accuracy: 0.722,  test  loss: 1.019\n",
            "\n",
            "Epoch 25 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.825,  train loss: 0.646\n",
            "test  accuracy: 0.706,  test  loss: 1.072\n",
            "\n",
            "train accuracy: 0.842,  train loss: 0.597\n",
            "test  accuracy: 0.721,  test  loss: 1.01\n",
            "\n",
            "Epoch 26 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.843,  train loss: 0.596\n",
            "test  accuracy: 0.728,  test  loss: 1.008\n",
            "\n",
            "train accuracy: 0.85,  train loss: 0.587\n",
            "test  accuracy: 0.724,  test  loss: 1.012\n",
            "\n",
            "Epoch 27 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.843,  train loss: 0.584\n",
            "test  accuracy: 0.724,  test  loss: 1.005\n",
            "\n",
            "train accuracy: 0.848,  train loss: 0.573\n",
            "test  accuracy: 0.718,  test  loss: 1.012\n",
            "\n",
            "Epoch 28 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.845,  train loss: 0.574\n",
            "test  accuracy: 0.718,  test  loss: 1.018\n",
            "\n",
            "train accuracy: 0.852,  train loss: 0.562\n",
            "test  accuracy: 0.717,  test  loss: 0.997\n",
            "\n",
            "Epoch 29 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.848,  train loss: 0.562\n",
            "test  accuracy: 0.716,  test  loss: 1.005\n",
            "\n",
            "train accuracy: 0.854,  train loss: 0.541\n",
            "test  accuracy: 0.726,  test  loss: 0.998\n",
            "\n",
            "Epoch 30 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.864,  train loss: 0.53\n",
            "test  accuracy: 0.724,  test  loss: 0.985\n",
            "\n",
            "train accuracy: 0.864,  train loss: 0.52\n",
            "test  accuracy: 0.723,  test  loss: 0.981\n",
            "\n",
            "Epoch 31 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.866,  train loss: 0.515\n",
            "test  accuracy: 0.731,  test  loss: 0.978\n",
            "\n",
            "train accuracy: 0.866,  train loss: 0.51\n",
            "test  accuracy: 0.722,  test  loss: 0.987\n",
            "\n",
            "Epoch 32 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.859,  train loss: 0.515\n",
            "test  accuracy: 0.726,  test  loss: 0.979\n",
            "\n",
            "train accuracy: 0.863,  train loss: 0.508\n",
            "test  accuracy: 0.722,  test  loss: 0.98\n",
            "\n",
            "Epoch 33 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.871,  train loss: 0.503\n",
            "test  accuracy: 0.727,  test  loss: 0.991\n",
            "\n",
            "train accuracy: 0.866,  train loss: 0.5\n",
            "test  accuracy: 0.734,  test  loss: 0.985\n",
            "\n",
            "Epoch 34 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.873,  train loss: 0.49\n",
            "test  accuracy: 0.726,  test  loss: 0.991\n",
            "\n",
            "train accuracy: 0.874,  train loss: 0.482\n",
            "test  accuracy: 0.729,  test  loss: 0.978\n",
            "\n",
            "Epoch 35 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.867,  train loss: 0.492\n",
            "test  accuracy: 0.727,  test  loss: 0.982\n",
            "\n",
            "train accuracy: 0.885,  train loss: 0.454\n",
            "test  accuracy: 0.737,  test  loss: 0.952\n",
            "\n",
            "Epoch 36 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.88,  train loss: 0.461\n",
            "test  accuracy: 0.737,  test  loss: 0.969\n",
            "\n",
            "train accuracy: 0.884,  train loss: 0.454\n",
            "test  accuracy: 0.733,  test  loss: 0.959\n",
            "\n",
            "Epoch 37 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.877,  train loss: 0.457\n",
            "test  accuracy: 0.734,  test  loss: 0.957\n",
            "\n",
            "train accuracy: 0.89,  train loss: 0.434\n",
            "test  accuracy: 0.74,  test  loss: 0.953\n",
            "\n",
            "Epoch 38 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.881,  train loss: 0.446\n",
            "test  accuracy: 0.733,  test  loss: 0.978\n",
            "\n",
            "train accuracy: 0.885,  train loss: 0.439\n",
            "test  accuracy: 0.735,  test  loss: 0.969\n",
            "\n",
            "Epoch 39 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.885,  train loss: 0.44\n",
            "test  accuracy: 0.73,  test  loss: 0.995\n",
            "\n",
            "train accuracy: 0.887,  train loss: 0.428\n",
            "test  accuracy: 0.733,  test  loss: 0.966\n",
            "\n",
            "Epoch 40 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.891,  train loss: 0.422\n",
            "test  accuracy: 0.738,  test  loss: 0.969\n",
            "\n",
            "train accuracy: 0.884,  train loss: 0.429\n",
            "test  accuracy: 0.729,  test  loss: 0.97\n",
            "\n",
            "Epoch 41 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.893,  train loss: 0.416\n",
            "test  accuracy: 0.733,  test  loss: 0.97\n",
            "\n",
            "train accuracy: 0.896,  train loss: 0.41\n",
            "test  accuracy: 0.735,  test  loss: 0.971\n",
            "\n",
            "Epoch 42 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.895,  train loss: 0.412\n",
            "test  accuracy: 0.731,  test  loss: 0.973\n",
            "\n",
            "train accuracy: 0.895,  train loss: 0.406\n",
            "test  accuracy: 0.734,  test  loss: 0.972\n",
            "\n",
            "Epoch 43 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.9,  train loss: 0.392\n",
            "test  accuracy: 0.744,  test  loss: 0.948\n",
            "\n",
            "train accuracy: 0.902,  train loss: 0.385\n",
            "test  accuracy: 0.743,  test  loss: 0.963\n",
            "\n",
            "Epoch 44 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.908,  train loss: 0.379\n",
            "test  accuracy: 0.739,  test  loss: 0.968\n",
            "\n",
            "train accuracy: 0.896,  train loss: 0.4\n",
            "test  accuracy: 0.737,  test  loss: 0.984\n",
            "\n",
            "Epoch 45 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.904,  train loss: 0.372\n",
            "test  accuracy: 0.737,  test  loss: 0.956\n",
            "\n",
            "train accuracy: 0.905,  train loss: 0.369\n",
            "test  accuracy: 0.742,  test  loss: 0.968\n",
            "\n",
            "Epoch 46 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.899,  train loss: 0.382\n",
            "test  accuracy: 0.73,  test  loss: 0.974\n",
            "\n",
            "train accuracy: 0.915,  train loss: 0.351\n",
            "test  accuracy: 0.749,  test  loss: 0.953\n",
            "\n",
            "Epoch 47 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.914,  train loss: 0.349\n",
            "test  accuracy: 0.745,  test  loss: 0.947\n",
            "\n",
            "train accuracy: 0.914,  train loss: 0.35\n",
            "test  accuracy: 0.745,  test  loss: 0.955\n",
            "\n",
            "Epoch 48 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.911,  train loss: 0.361\n",
            "test  accuracy: 0.73,  test  loss: 0.987\n",
            "\n",
            "train accuracy: 0.913,  train loss: 0.335\n",
            "test  accuracy: 0.747,  test  loss: 0.951\n",
            "\n",
            "Epoch 49 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.91,  train loss: 0.352\n",
            "test  accuracy: 0.74,  test  loss: 0.973\n",
            "\n",
            "train accuracy: 0.909,  train loss: 0.346\n",
            "test  accuracy: 0.736,  test  loss: 0.982\n",
            "\n",
            "Epoch 50 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.919,  train loss: 0.329\n",
            "test  accuracy: 0.743,  test  loss: 0.949\n",
            "\n",
            "train accuracy: 0.915,  train loss: 0.345\n",
            "test  accuracy: 0.735,  test  loss: 0.986\n",
            "\n",
            "Epoch 51 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.924,  train loss: 0.318\n",
            "test  accuracy: 0.74,  test  loss: 0.947\n",
            "\n",
            "train accuracy: 0.915,  train loss: 0.333\n",
            "test  accuracy: 0.739,  test  loss: 0.972\n",
            "\n",
            "Epoch 52 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.919,  train loss: 0.326\n",
            "test  accuracy: 0.745,  test  loss: 0.978\n",
            "\n",
            "train accuracy: 0.918,  train loss: 0.321\n",
            "test  accuracy: 0.745,  test  loss: 0.968\n",
            "\n",
            "Epoch 53 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.922,  train loss: 0.326\n",
            "test  accuracy: 0.738,  test  loss: 0.988\n",
            "\n",
            "train accuracy: 0.914,  train loss: 0.326\n",
            "test  accuracy: 0.742,  test  loss: 0.988\n",
            "\n",
            "Epoch 54 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.928,  train loss: 0.299\n",
            "test  accuracy: 0.745,  test  loss: 0.971\n",
            "\n",
            "train accuracy: 0.928,  train loss: 0.291\n",
            "test  accuracy: 0.749,  test  loss: 0.952\n",
            "\n",
            "Epoch 55 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.918,  train loss: 0.317\n",
            "test  accuracy: 0.734,  test  loss: 0.997\n",
            "\n",
            "train accuracy: 0.93,  train loss: 0.289\n",
            "test  accuracy: 0.752,  test  loss: 0.949\n",
            "\n",
            "Epoch 56 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.934,  train loss: 0.287\n",
            "test  accuracy: 0.74,  test  loss: 0.97\n",
            "\n",
            "train accuracy: 0.934,  train loss: 0.289\n",
            "test  accuracy: 0.744,  test  loss: 0.973\n",
            "\n",
            "Epoch 57 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.934,  train loss: 0.277\n",
            "test  accuracy: 0.747,  test  loss: 0.974\n",
            "\n",
            "train accuracy: 0.933,  train loss: 0.275\n",
            "test  accuracy: 0.748,  test  loss: 0.96\n",
            "\n",
            "Epoch 58 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.935,  train loss: 0.273\n",
            "test  accuracy: 0.741,  test  loss: 0.984\n",
            "\n",
            "train accuracy: 0.935,  train loss: 0.274\n",
            "test  accuracy: 0.751,  test  loss: 0.97\n",
            "\n",
            "Epoch 59 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.923,  train loss: 0.295\n",
            "test  accuracy: 0.74,  test  loss: 1.0\n",
            "\n",
            "train accuracy: 0.935,  train loss: 0.269\n",
            "test  accuracy: 0.757,  test  loss: 0.957\n",
            "\n",
            "Epoch 60 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.939,  train loss: 0.264\n",
            "test  accuracy: 0.751,  test  loss: 0.976\n",
            "\n",
            "train accuracy: 0.943,  train loss: 0.255\n",
            "test  accuracy: 0.754,  test  loss: 0.95\n",
            "\n",
            "Epoch 61 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.937,  train loss: 0.267\n",
            "test  accuracy: 0.739,  test  loss: 0.998\n",
            "\n",
            "train accuracy: 0.941,  train loss: 0.253\n",
            "test  accuracy: 0.746,  test  loss: 0.981\n",
            "\n",
            "Epoch 62 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.942,  train loss: 0.249\n",
            "test  accuracy: 0.745,  test  loss: 0.979\n",
            "\n",
            "train accuracy: 0.935,  train loss: 0.258\n",
            "test  accuracy: 0.737,  test  loss: 1.006\n",
            "\n",
            "Epoch 63 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.938,  train loss: 0.252\n",
            "test  accuracy: 0.743,  test  loss: 1.001\n",
            "\n",
            "train accuracy: 0.949,  train loss: 0.237\n",
            "test  accuracy: 0.748,  test  loss: 0.986\n",
            "\n",
            "Epoch 64 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.934,  train loss: 0.255\n",
            "test  accuracy: 0.745,  test  loss: 0.988\n",
            "\n",
            "train accuracy: 0.94,  train loss: 0.244\n",
            "test  accuracy: 0.745,  test  loss: 1.002\n",
            "\n",
            "Epoch 65 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.945,  train loss: 0.238\n",
            "test  accuracy: 0.743,  test  loss: 1.004\n",
            "\n",
            "train accuracy: 0.944,  train loss: 0.235\n",
            "test  accuracy: 0.747,  test  loss: 1.005\n",
            "\n",
            "Epoch 66 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.942,  train loss: 0.233\n",
            "test  accuracy: 0.75,  test  loss: 0.989\n",
            "\n",
            "train accuracy: 0.938,  train loss: 0.246\n",
            "test  accuracy: 0.74,  test  loss: 1.05\n",
            "\n",
            "Epoch 67 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.943,  train loss: 0.231\n",
            "test  accuracy: 0.759,  test  loss: 0.972\n",
            "\n",
            "train accuracy: 0.948,  train loss: 0.223\n",
            "test  accuracy: 0.744,  test  loss: 1.001\n",
            "\n",
            "Epoch 68 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.95,  train loss: 0.219\n",
            "test  accuracy: 0.751,  test  loss: 0.998\n",
            "\n",
            "train accuracy: 0.951,  train loss: 0.218\n",
            "test  accuracy: 0.749,  test  loss: 1.008\n",
            "\n",
            "Epoch 69 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.954,  train loss: 0.205\n",
            "test  accuracy: 0.753,  test  loss: 0.977\n",
            "\n",
            "train accuracy: 0.95,  train loss: 0.216\n",
            "test  accuracy: 0.751,  test  loss: 1.0\n",
            "\n",
            "Epoch 70 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.951,  train loss: 0.208\n",
            "test  accuracy: 0.75,  test  loss: 0.999\n",
            "\n",
            "train accuracy: 0.952,  train loss: 0.206\n",
            "test  accuracy: 0.751,  test  loss: 0.998\n",
            "\n",
            "Epoch 71 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.954,  train loss: 0.204\n",
            "test  accuracy: 0.745,  test  loss: 1.01\n",
            "\n",
            "train accuracy: 0.955,  train loss: 0.196\n",
            "test  accuracy: 0.745,  test  loss: 1.015\n",
            "\n",
            "Epoch 72 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.95,  train loss: 0.206\n",
            "test  accuracy: 0.747,  test  loss: 0.997\n",
            "\n",
            "train accuracy: 0.953,  train loss: 0.197\n",
            "test  accuracy: 0.747,  test  loss: 1.018\n",
            "\n",
            "Epoch 73 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.954,  train loss: 0.199\n",
            "test  accuracy: 0.752,  test  loss: 0.999\n",
            "\n",
            "train accuracy: 0.955,  train loss: 0.194\n",
            "test  accuracy: 0.747,  test  loss: 1.021\n",
            "\n",
            "Epoch 74 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.958,  train loss: 0.189\n",
            "test  accuracy: 0.746,  test  loss: 1.015\n",
            "\n",
            "train accuracy: 0.962,  train loss: 0.184\n",
            "test  accuracy: 0.749,  test  loss: 1.019\n",
            "\n",
            "Epoch 75 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.955,  train loss: 0.189\n",
            "test  accuracy: 0.75,  test  loss: 1.034\n",
            "\n",
            "train accuracy: 0.963,  train loss: 0.175\n",
            "test  accuracy: 0.746,  test  loss: 1.022\n",
            "\n",
            "Epoch 76 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.959,  train loss: 0.179\n",
            "test  accuracy: 0.746,  test  loss: 1.012\n",
            "\n",
            "train accuracy: 0.953,  train loss: 0.188\n",
            "test  accuracy: 0.738,  test  loss: 1.068\n",
            "\n",
            "Epoch 77 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.963,  train loss: 0.18\n",
            "test  accuracy: 0.744,  test  loss: 1.031\n",
            "\n",
            "train accuracy: 0.966,  train loss: 0.163\n",
            "test  accuracy: 0.75,  test  loss: 1.018\n",
            "\n",
            "Epoch 78 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.961,  train loss: 0.175\n",
            "test  accuracy: 0.755,  test  loss: 1.022\n",
            "\n",
            "train accuracy: 0.965,  train loss: 0.171\n",
            "test  accuracy: 0.753,  test  loss: 1.036\n",
            "\n",
            "Epoch 79 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.959,  train loss: 0.175\n",
            "test  accuracy: 0.75,  test  loss: 1.053\n",
            "\n",
            "train accuracy: 0.968,  train loss: 0.157\n",
            "test  accuracy: 0.755,  test  loss: 1.017\n",
            "\n",
            "Epoch 80 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.966,  train loss: 0.159\n",
            "test  accuracy: 0.751,  test  loss: 1.016\n",
            "\n",
            "train accuracy: 0.961,  train loss: 0.168\n",
            "test  accuracy: 0.746,  test  loss: 1.063\n",
            "\n",
            "Epoch 81 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.965,  train loss: 0.16\n",
            "test  accuracy: 0.747,  test  loss: 1.053\n",
            "\n",
            "train accuracy: 0.972,  train loss: 0.146\n",
            "test  accuracy: 0.75,  test  loss: 1.04\n",
            "\n",
            "Epoch 82 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.974,  train loss: 0.147\n",
            "test  accuracy: 0.754,  test  loss: 1.034\n",
            "\n",
            "train accuracy: 0.958,  train loss: 0.177\n",
            "test  accuracy: 0.746,  test  loss: 1.083\n",
            "\n",
            "Epoch 83 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.965,  train loss: 0.158\n",
            "test  accuracy: 0.744,  test  loss: 1.079\n",
            "\n",
            "train accuracy: 0.966,  train loss: 0.159\n",
            "test  accuracy: 0.749,  test  loss: 1.071\n",
            "\n",
            "Epoch 84 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.963,  train loss: 0.158\n",
            "test  accuracy: 0.75,  test  loss: 1.072\n",
            "\n",
            "train accuracy: 0.973,  train loss: 0.139\n",
            "test  accuracy: 0.757,  test  loss: 1.047\n",
            "\n",
            "Epoch 85 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.959,  train loss: 0.169\n",
            "test  accuracy: 0.737,  test  loss: 1.116\n",
            "\n",
            "train accuracy: 0.973,  train loss: 0.144\n",
            "test  accuracy: 0.747,  test  loss: 1.074\n",
            "\n",
            "Epoch 86 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.966,  train loss: 0.15\n",
            "test  accuracy: 0.744,  test  loss: 1.101\n",
            "\n",
            "train accuracy: 0.973,  train loss: 0.137\n",
            "test  accuracy: 0.756,  test  loss: 1.044\n",
            "\n",
            "Epoch 87 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.967,  train loss: 0.15\n",
            "test  accuracy: 0.746,  test  loss: 1.076\n",
            "\n",
            "train accuracy: 0.972,  train loss: 0.134\n",
            "test  accuracy: 0.749,  test  loss: 1.066\n",
            "\n",
            "Epoch 88 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.96,  train loss: 0.158\n",
            "test  accuracy: 0.74,  test  loss: 1.115\n",
            "\n",
            "train accuracy: 0.975,  train loss: 0.129\n",
            "test  accuracy: 0.745,  test  loss: 1.088\n",
            "\n",
            "Epoch 89 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.974,  train loss: 0.133\n",
            "test  accuracy: 0.75,  test  loss: 1.068\n",
            "\n",
            "train accuracy: 0.973,  train loss: 0.128\n",
            "test  accuracy: 0.756,  test  loss: 1.091\n",
            "\n",
            "Epoch 90 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.973,  train loss: 0.128\n",
            "test  accuracy: 0.754,  test  loss: 1.077\n",
            "\n",
            "train accuracy: 0.975,  train loss: 0.126\n",
            "test  accuracy: 0.756,  test  loss: 1.077\n",
            "\n",
            "Epoch 91 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.971,  train loss: 0.137\n",
            "test  accuracy: 0.752,  test  loss: 1.129\n",
            "\n",
            "train accuracy: 0.976,  train loss: 0.121\n",
            "test  accuracy: 0.75,  test  loss: 1.068\n",
            "\n",
            "Epoch 92 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.978,  train loss: 0.116\n",
            "test  accuracy: 0.755,  test  loss: 1.071\n",
            "\n",
            "train accuracy: 0.975,  train loss: 0.122\n",
            "test  accuracy: 0.75,  test  loss: 1.123\n",
            "\n",
            "Epoch 93 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.971,  train loss: 0.135\n",
            "test  accuracy: 0.749,  test  loss: 1.118\n",
            "\n",
            "train accuracy: 0.976,  train loss: 0.123\n",
            "test  accuracy: 0.743,  test  loss: 1.124\n",
            "\n",
            "Epoch 94 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.969,  train loss: 0.135\n",
            "test  accuracy: 0.748,  test  loss: 1.117\n",
            "\n",
            "train accuracy: 0.982,  train loss: 0.107\n",
            "test  accuracy: 0.753,  test  loss: 1.095\n",
            "\n",
            "Epoch 95 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.982,  train loss: 0.108\n",
            "test  accuracy: 0.754,  test  loss: 1.104\n",
            "\n",
            "train accuracy: 0.977,  train loss: 0.118\n",
            "test  accuracy: 0.748,  test  loss: 1.137\n",
            "\n",
            "Epoch 96 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.972,  train loss: 0.125\n",
            "test  accuracy: 0.745,  test  loss: 1.153\n",
            "\n",
            "train accuracy: 0.982,  train loss: 0.105\n",
            "test  accuracy: 0.746,  test  loss: 1.121\n",
            "\n",
            "Epoch 97 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.983,  train loss: 0.102\n",
            "test  accuracy: 0.751,  test  loss: 1.1\n",
            "\n",
            "train accuracy: 0.983,  train loss: 0.101\n",
            "test  accuracy: 0.756,  test  loss: 1.122\n",
            "\n",
            "Epoch 98 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.984,  train loss: 0.096\n",
            "test  accuracy: 0.756,  test  loss: 1.105\n",
            "\n",
            "train accuracy: 0.983,  train loss: 0.101\n",
            "test  accuracy: 0.747,  test  loss: 1.14\n",
            "\n",
            "Epoch 99 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.973,  train loss: 0.116\n",
            "test  accuracy: 0.752,  test  loss: 1.118\n",
            "\n",
            "train accuracy: 0.977,  train loss: 0.111\n",
            "test  accuracy: 0.749,  test  loss: 1.166\n",
            "\n",
            "Epoch 100 ---------------------------------- \n",
            "\n",
            "train accuracy: 0.974,  train loss: 0.11\n",
            "test  accuracy: 0.75,  test  loss: 1.119\n",
            "\n",
            "train accuracy: 0.984,  train loss: 0.096\n",
            "test  accuracy: 0.754,  test  loss: 1.13\n",
            "\n",
            " ~~~~~ training is done ~~~~~\n",
            "\n",
            "CPU times: user 1h 50min 29s, sys: 38 s, total: 1h 51min 7s\n",
            "Wall time: 1h 51min 37s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(A_Train, label='train set')\n",
        "plt.plot(A_Test, label='test set')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(L_Train, label='train set')\n",
        "plt.plot(L_Test, label='test set')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2GoaFOS5BxYX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "87325409-74e1-483a-c848-984705677947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-48f6ad349eb6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_Test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.choice(len(test_dataset))\n",
        "\n",
        "x = test_dataset[idx][0]\n",
        "print(f'x of {x.shape} :')\n",
        "plt.imshow(x[0], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(f'true label = y = {test_dataset[idx][1]}\\n')\n",
        "\n",
        "\n",
        "out_ = model(x.view(1, 1, 28, 28)).detach().flatten()\n",
        "prob = F.softmax(out_, dim=0)\n",
        "pred = prob.argmax().item()\n",
        "print(f'predicted label = {pred}\\n')\n",
        "\n",
        "plt.stem(np.arange(10), prob)\n",
        "plt.ylabel('probability')\n",
        "plt.xlabel('class labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nHP5SDoACIXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2048-1896"
      ],
      "metadata": {
        "id": "ALbuTf98CKjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XY7BGRpnp26I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}